{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10508636,"sourceType":"datasetVersion","datasetId":6505884},{"sourceId":11419113,"sourceType":"datasetVersion","datasetId":6529307},{"sourceId":11545904,"sourceType":"datasetVersion","datasetId":6529314},{"sourceId":235859085,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, Subset, BatchSampler, WeightedRandomSampler\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models import VGG16_Weights\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time","metadata":{"id":"x5ee94tk9wcf","outputId":"fa8453f8-e4a0-4e0c-fa41-24aeba9ae9fc","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.305672Z","iopub.execute_input":"2025-06-18T16:41:03.305981Z","iopub.status.idle":"2025-06-18T16:41:03.310931Z","shell.execute_reply.started":"2025-06-18T16:41:03.305954Z","shell.execute_reply":"2025-06-18T16:41:03.310019Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.328585Z","iopub.execute_input":"2025-06-18T16:41:03.328862Z","iopub.status.idle":"2025-06-18T16:41:03.333644Z","shell.execute_reply.started":"2025-06-18T16:41:03.328833Z","shell.execute_reply":"2025-06-18T16:41:03.332734Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# DATA PREPARATION","metadata":{}},{"cell_type":"code","source":"class MultiInputDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform if transform else get_default_transform()\n        self.samples = []\n        self.labels = []\n        self.label_map = {'KCN':0, 'NOR':1, 'SUSP':2}\n\n        #class loop\n        for class_name in os.listdir(root_dir):\n            class_path = os.path.join(root_dir, class_name)\n            if not os.path.isdir(class_path):\n                continue\n            for case_name in os.listdir(class_path):\n                case_path = os.path.join(class_path, case_name)\n                if os.path.isdir(case_path):\n                    #pick prefix\n                    sample_files = os.listdir(case_path)\n                    if sample_files:\n                        prefix = sample_files[0].split('_')[0] \n                        case_number = sample_files[0].split('_')[1]\n                        case_prefix = f\"{prefix}_{case_number}\"\n                        self.samples.append((case_path, case_prefix, prefix))\n                        self.labels.append(self.label_map[prefix])\n        self.labels = np.array(self.labels)\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        case_path, case_prefix, prefix = self.samples[idx]\n\n        #akhiran nama file\n        feature_suffixes = [\n            \"_CT_A.jpg\", \"_EC_A.jpg\", \"_EC_P.jpg\", \"_Elv_A.jpg\",\"_Elv_P.jpg\", \"_Sag_A.jpg\", \"_Sag_P.jpg\"\n        ]\n\n        feature_images = []\n        for suffix in feature_suffixes :\n            filename = f\"{case_prefix}{suffix}\"\n            img_path = os.path.join(case_path, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            img = self.transform(img)\n            feature_images.append(img)\n\n        #shape : (7, 3, H, W)\n        stacked = torch.stack(feature_images, dim=0)\n        label = self.label_map[prefix]\n        return stacked, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.334723Z","iopub.execute_input":"2025-06-18T16:41:03.334967Z","iopub.status.idle":"2025-06-18T16:41:03.348200Z","shell.execute_reply.started":"2025-06-18T16:41:03.334937Z","shell.execute_reply":"2025-06-18T16:41:03.347426Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def get_default_transform():\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.349855Z","iopub.execute_input":"2025-06-18T16:41:03.350094Z","iopub.status.idle":"2025-06-18T16:41:03.361004Z","shell.execute_reply.started":"2025-06-18T16:41:03.350074Z","shell.execute_reply":"2025-06-18T16:41:03.360224Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def load_dataset(data_dir, transform=None):\n    #create and return teh dataset instance\n    try :\n        dataset = MultiInputDataset(data_dir, transform)\n        print(f'Dataset loaded succesfully with {len(dataset)} samples')\n        return dataset\n    except Exception as e:\n        print(f'Error loading dataset : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.361907Z","iopub.execute_input":"2025-06-18T16:41:03.362109Z","iopub.status.idle":"2025-06-18T16:41:03.413514Z","shell.execute_reply.started":"2025-06-18T16:41:03.362091Z","shell.execute_reply":"2025-06-18T16:41:03.412736Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def create_train_val_split(dataset, val_size=0.2, random_state=42):\n    #splitting dataset with statification\n    try :\n        train_idx, val_idx = train_test_split(\n            range(len(dataset)),\n            test_size = val_size,\n            stratify = dataset.labels,\n            random_state = random_state\n        )\n        print(f'Split created : {len(train_idx)} training samples ; {len(val_idx)} validation samples')\n        return train_idx, val_idx\n    except Exception as e:\n        print(f'Error Splitting : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.414313Z","iopub.execute_input":"2025-06-18T16:41:03.414578Z","iopub.status.idle":"2025-06-18T16:41:03.425510Z","shell.execute_reply.started":"2025-06-18T16:41:03.414546Z","shell.execute_reply":"2025-06-18T16:41:03.424726Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"def create_dataloaders(train_dataset, val_dataset, batch_size=8, num_workers=2):\n    #create and return train and val dataloaders\n    try :\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size = batch_size,\n            shuffle = True,\n            num_workers = num_workers,\n            pin_memory = True\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size = batch_size,\n            shuffle = False,\n            num_workers = num_workers,\n            pin_memory = True\n        )\n        print(f'DataLoaders created with batch size {batch_size}')\n        return train_loader, val_loader\n    except Exception as e:\n        print(f'Error creating DataLoaders : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.426308Z","iopub.execute_input":"2025-06-18T16:41:03.426599Z","iopub.status.idle":"2025-06-18T16:41:03.436049Z","shell.execute_reply.started":"2025-06-18T16:41:03.426569Z","shell.execute_reply":"2025-06-18T16:41:03.435376Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"# def prepare_dataloaders(data_dir, batch_size=8, test_size=0.2, random_state=42):\n\n#     print(f'Loading dara from : {os.path.abspath(data_dir)}')\n#     transform = get_default_transform()\n#     full_dataset = MultiInputDataset(root_dir = data_dir, transform=transform)\n#     print(f\"Dataset created with {len(full_dataset)} samples\")\n#     if len(full_dataset) == 0:\n#         print(f'WARNING : Dataset is empty!')\n#         return None, None\n\n#     train_idx, val_idx = train_test_split(\n#         list(range(len(full_dataset))),\n#         test_size=test_size,\n#         random_state=random_state,\n#         shuffle=True\n#     )\n\n#     train_dataset = Subset(full_dataset, train_idx)\n#     val_dataset = Subset(full_dataset, val_idx)\n\n#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n#     return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.495551Z","iopub.execute_input":"2025-06-18T16:41:03.495748Z","iopub.status.idle":"2025-06-18T16:41:03.498978Z","shell.execute_reply.started":"2025-06-18T16:41:03.495731Z","shell.execute_reply":"2025-06-18T16:41:03.498230Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"def test_sample_batch(loader):\n    images, labels = next(iter(loader))\n    print(\"Batch image shape : \", images.shape)\n    print(\"Labels : \", labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.499969Z","iopub.execute_input":"2025-06-18T16:41:03.500186Z","iopub.status.idle":"2025-06-18T16:41:03.513540Z","shell.execute_reply.started":"2025-06-18T16:41:03.500155Z","shell.execute_reply":"2025-06-18T16:41:03.512701Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# data_dir = \"/kaggle/input/cornealtopography/Train_Validation sets/Train_Validation sets\"\n\n# print(\"Folders : \", os.listdir(data_dir))\n# for class_name in os.listdir(data_dir):\n#     class_path = os.path.join(data_dir, class_name)\n#     print(f\"{class_name} -> {len(os.listdir(class_path))} case\")\n\n# train_loader, val_loader = prepare_dataloaders(data_dir, batch_size=8)\n# test_sample_batch(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.515268Z","iopub.execute_input":"2025-06-18T16:41:03.515569Z","iopub.status.idle":"2025-06-18T16:41:03.524905Z","shell.execute_reply.started":"2025-06-18T16:41:03.515540Z","shell.execute_reply":"2025-06-18T16:41:03.524115Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":" # MODEL SETUP","metadata":{}},{"cell_type":"code","source":"def get_model_vgg16(num_classes = 3, device = None):\n\n    print(f'Preparing VGG16 for {num_classes} classes...')\n    vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n    for param in vgg16.parameters():\n        param.requires_grad = False\n    vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n    for param in vgg16.classifier.parameters():\n        param.requires_grad = True\n    if device is not None:\n        vgg16 = vgg16.to(device)\n    print(f'Model ready!')\n\n    return vgg16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.525841Z","iopub.execute_input":"2025-06-18T16:41:03.526034Z","iopub.status.idle":"2025-06-18T16:41:03.537990Z","shell.execute_reply.started":"2025-06-18T16:41:03.526017Z","shell.execute_reply":"2025-06-18T16:41:03.537079Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"def main() : \n    data_dir = \"/kaggle/input/cornealtopography/Train_Validation sets/Train_Validation sets\"\n    try :\n        print(\"Folders : \", os.listdir(data_dir))\n        for class_name in os.listdir(data_dir):\n            class_path = os.path.join(data_dir, class_name)\n            print(f\"{class_name} -> {len(os.listdir(class_path))} case\")\n\n        #data preparation\n        dataset = load_dataset(data_dir)\n        train_idx, val_idx = create_train_val_split(dataset)\n\n        train_dataset = Subset(dataset, train_idx)\n        val_dataset = Subset(dataset, val_idx)\n\n        train_loader, val_loader = create_dataloaders(train_dataset, val_dataset)\n\n        test_sample_batch (train_loader)\n\n        #model setup\n        model = get_model_vgg16(num_classes = 3, device=device)\n        \n        return train_loader, val_loader, model\n\n    except Exception as e :\n        print(f'Error in data preparation : {str(e)}')\n        return None, None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.538848Z","iopub.execute_input":"2025-06-18T16:41:03.539076Z","iopub.status.idle":"2025-06-18T16:41:03.552895Z","shell.execute_reply.started":"2025-06-18T16:41:03.539052Z","shell.execute_reply":"2025-06-18T16:41:03.552122Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"if __name__ == \"__main__\" :\n    traind_loader, val_loader, model = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:41:03.553697Z","iopub.execute_input":"2025-06-18T16:41:03.553927Z","iopub.status.idle":"2025-06-18T16:41:06.467985Z","shell.execute_reply.started":"2025-06-18T16:41:03.553902Z","shell.execute_reply":"2025-06-18T16:41:06.466886Z"}},"outputs":[{"name":"stdout","text":"Folders :  ['Keratoconus', 'Normal', 'Suspect']\nKeratoconus -> 150 case\nNormal -> 150 case\nSuspect -> 123 case\nDataset loaded succesfully with 423 samples\nSplit created : 338 training samples ; 85 validation samples\nDataLoaders created with batch size 8\nBatch image shape :  torch.Size([8, 7, 3, 224, 224])\nLabels :  tensor([2, 0, 1, 0, 2, 0, 0, 1])\nPreparing VGG16 for 3 classes...\nModel ready!\n","output_type":"stream"}],"execution_count":65}]}