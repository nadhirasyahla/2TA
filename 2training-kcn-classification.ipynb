{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10508636,"sourceType":"datasetVersion","datasetId":6505884},{"sourceId":11419113,"sourceType":"datasetVersion","datasetId":6529307},{"sourceId":11545904,"sourceType":"datasetVersion","datasetId":6529314},{"sourceId":235859085,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, Subset, BatchSampler, WeightedRandomSampler\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models import VGG16_Weights\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time","metadata":{"id":"x5ee94tk9wcf","outputId":"fa8453f8-e4a0-4e0c-fa41-24aeba9ae9fc","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:15:55.082596Z","iopub.execute_input":"2025-06-05T08:15:55.082847Z","iopub.status.idle":"2025-06-05T08:16:01.163447Z","shell.execute_reply.started":"2025-06-05T08:15:55.082815Z","shell.execute_reply":"2025-06-05T08:16:01.162731Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.164175Z","iopub.execute_input":"2025-06-05T08:16:01.164470Z","iopub.status.idle":"2025-06-05T08:16:01.238086Z","shell.execute_reply.started":"2025-06-05T08:16:01.164450Z","shell.execute_reply":"2025-06-05T08:16:01.237108Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# DATA PREPARATION","metadata":{}},{"cell_type":"code","source":"class MultiInputDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.samples = []\n        self.labels = []\n        self.label_map = {'KCN':0, 'NOR':1, 'SUSP':2}\n\n        #class loop\n        for class_name in os.listdir(root_dir):\n            class_path = os.path.join(root_dir, class_name)\n            if not os.path.isdir(class_path):\n                continue\n            for case_name in os.listdir(class_path):\n                case_path = os.path.join(class_path, case_name)\n                if os.path.isdir(case_path):\n                    #pick prefix\n                    sample_files = os.listdir(case_path)\n                    if sample_files:\n                        prefix = sample_files[0].split('_')[0] \n                        case_number = sample_files[0].split('_')[1]\n                        case_prefix = f\"{prefix}_{case_number}\"\n                        self.samples.append((case_path, case_prefix, prefix))\n                        self.labels.append(self.label_map[prefix])\n        self.labels = np.array(self.labels)\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        case_path, case_prefix, prefix = self.samples[idx]\n\n        #akhiran nama file\n        feature_suffixes = [\n            \"_CT_A.jpg\", \"_EC_A.jpg\", \"_EC_P.jpg\", \"_Elv_A.jpg\",\"_Elv_P.jpg\", \"_Sag_A.jpg\", \"_Sag_P.jpg\"\n        ]\n\n        feature_images = []\n        for suffix in feature_suffixes :\n            filename = f\"{case_prefix}{suffix}\"\n            img_path = os.path.join(case_path, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            if self.transform :\n                img = self.transform(img)\n            feature_images.append(img)\n\n        #shape : (7, 3, H, W)\n        stacked = torch.stack(feature_images, dim=0)\n        label = self.label_map[prefix]\n        return stacked, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.239039Z","iopub.execute_input":"2025-06-05T08:16:01.239302Z","iopub.status.idle":"2025-06-05T08:16:01.255548Z","shell.execute_reply.started":"2025-06-05T08:16:01.239267Z","shell.execute_reply":"2025-06-05T08:16:01.254623Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_default_transform():\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.257296Z","iopub.execute_input":"2025-06-05T08:16:01.257532Z","iopub.status.idle":"2025-06-05T08:16:01.271640Z","shell.execute_reply.started":"2025-06-05T08:16:01.257511Z","shell.execute_reply":"2025-06-05T08:16:01.270928Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_dataset(data_dir, transform=None):\n    #create and return teh dataset instance\n    try :\n        dataset = MultiInputDataset(data_dir, transform)\n        print(f'Dataset loaded succesfully with {len(dataset)} samples')\n        return dataset\n    except Exception as e:\n        print(f'Error loading dataset : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.272836Z","iopub.execute_input":"2025-06-05T08:16:01.273077Z","iopub.status.idle":"2025-06-05T08:16:01.287836Z","shell.execute_reply.started":"2025-06-05T08:16:01.273054Z","shell.execute_reply":"2025-06-05T08:16:01.287040Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def create_train_val_split(dataset, val_size=0.2, random_state=42):\n    #splitting dataset with statification\n    try :\n        train_idx, val_idx = train_test_split(\n            range(len(dataset)),\n            test_size = val_size,\n            stratify = dataset.labels,\n            random_state = random_state\n        )\n        print(f'Split created : {len(train_idx)} training samples ; {len(val_idx)} validation samples')\n        return train_idx, val_idx\n    except Exception as e:\n        print(f'Error Splitting : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.288668Z","iopub.execute_input":"2025-06-05T08:16:01.288932Z","iopub.status.idle":"2025-06-05T08:16:01.299922Z","shell.execute_reply.started":"2025-06-05T08:16:01.288912Z","shell.execute_reply":"2025-06-05T08:16:01.299262Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_dataloaders(train_dataset, val_dataset, batch_size=8, num_workers=2):\n    #create and return train and val dataloaders\n    try :\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size = batch_size,\n            shuffle = True,\n            num_workers = num_workers,\n            pin_memory = True\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size = batch_size,\n            shuffle = False,\n            num_workers = num_workers,\n            pin_memory = True\n        )\n        print(f'DataLoaders created with batch size {batch_size}')\n        return train_loader, val_loader\n    except Exception as e:\n        print(f'Error creating DataLoaders : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.300663Z","iopub.execute_input":"2025-06-05T08:16:01.300922Z","iopub.status.idle":"2025-06-05T08:16:01.311578Z","shell.execute_reply.started":"2025-06-05T08:16:01.300902Z","shell.execute_reply":"2025-06-05T08:16:01.310754Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# def prepare_dataloaders(data_dir, batch_size=8, test_size=0.2, random_state=42):\n\n#     print(f'Loading dara from : {os.path.abspath(data_dir)}')\n#     transform = get_default_transform()\n#     full_dataset = MultiInputDataset(root_dir = data_dir, transform=transform)\n#     print(f\"Dataset created with {len(full_dataset)} samples\")\n#     if len(full_dataset) == 0:\n#         print(f'WARNING : Dataset is empty!')\n#         return None, None\n\n#     train_idx, val_idx = train_test_split(\n#         list(range(len(full_dataset))),\n#         test_size=test_size,\n#         random_state=random_state,\n#         shuffle=True\n#     )\n\n#     train_dataset = Subset(full_dataset, train_idx)\n#     val_dataset = Subset(full_dataset, val_idx)\n\n#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n#     return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.312363Z","iopub.execute_input":"2025-06-05T08:16:01.312616Z","iopub.status.idle":"2025-06-05T08:16:01.322784Z","shell.execute_reply.started":"2025-06-05T08:16:01.312590Z","shell.execute_reply":"2025-06-05T08:16:01.322009Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def test_sample_batch(loader):\n    images, labels = next(iter(loader))\n    print(\"Batch image shape : \", images.shape)\n    print(\"Labels : \", labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.323485Z","iopub.execute_input":"2025-06-05T08:16:01.323669Z","iopub.status.idle":"2025-06-05T08:16:01.335776Z","shell.execute_reply.started":"2025-06-05T08:16:01.323652Z","shell.execute_reply":"2025-06-05T08:16:01.335124Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# data_dir = \"/kaggle/input/cornealtopography/Train_Validation sets/Train_Validation sets\"\n\n# print(\"Folders : \", os.listdir(data_dir))\n# for class_name in os.listdir(data_dir):\n#     class_path = os.path.join(data_dir, class_name)\n#     print(f\"{class_name} -> {len(os.listdir(class_path))} case\")\n\n# train_loader, val_loader = prepare_dataloaders(data_dir, batch_size=8)\n# test_sample_batch(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.336394Z","iopub.execute_input":"2025-06-05T08:16:01.336591Z","iopub.status.idle":"2025-06-05T08:16:01.345795Z","shell.execute_reply.started":"2025-06-05T08:16:01.336573Z","shell.execute_reply":"2025-06-05T08:16:01.345129Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def main() : \n    data_dir = \"/kaggle/input/cornealtopography/Train_Validation sets/Train_Validation sets\"\n    try :\n        print(\"Folders : \", os.listdir(data_dir))\n        for class_name in os.listdir(data_dir):\n            class_path = os.path.join(data_dir, class_name)\n            print(f\"{class_name} -> {len(os.listdir(class_path))} case\")\n\n        dataset = load_dataset(data_dir)\n        train_idx, val_idx = create_train_val_split(dataset)\n\n        train_dataset = Subset(dataset, train_idx)\n        val_dataset = Subset(dataset, val_idx)\n\n        train_loader, val_loader = create_dataloaders(train_dataset, val_dataset)\n\n        test_sample_batch (train_loader)\n        return train_loader, val_loader\n\n    except Exception as e :\n        print(f'Error in data preparation : {str(e)}')\n        return None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.346659Z","iopub.execute_input":"2025-06-05T08:16:01.346964Z","iopub.status.idle":"2025-06-05T08:16:01.357301Z","shell.execute_reply.started":"2025-06-05T08:16:01.346935Z","shell.execute_reply":"2025-06-05T08:16:01.356534Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if __name__ == \"__main__\" :\n    traind_loader, val_loader = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:16:01.358285Z","iopub.execute_input":"2025-06-05T08:16:01.358599Z","iopub.status.idle":"2025-06-05T08:16:04.323942Z","shell.execute_reply.started":"2025-06-05T08:16:01.358570Z","shell.execute_reply":"2025-06-05T08:16:04.322681Z"}},"outputs":[{"name":"stdout","text":"Folders :  ['Keratoconus', 'Normal', 'Suspect']\nKeratoconus -> 150 case\nNormal -> 150 case\nSuspect -> 123 case\nDataset loaded succesfully with 423 samples\nSplit created : 338 training samples ; 85 validation samples\nDataLoaders created with batch size 8\nError in data preparation : Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-3-9786f553b115>\", line 47, in __getitem__\n    stacked = torch.stack(feature_images, dim=0)\nTypeError: expected Tensor as element 0 in argument 0, but got Image\n\n","output_type":"stream"}],"execution_count":12}]}