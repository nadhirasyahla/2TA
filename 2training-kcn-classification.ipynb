{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10508636,"sourceType":"datasetVersion","datasetId":6505884},{"sourceId":11419113,"sourceType":"datasetVersion","datasetId":6529307},{"sourceId":11545904,"sourceType":"datasetVersion","datasetId":6529314},{"sourceId":235859085,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, Subset, BatchSampler, WeightedRandomSampler\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models import VGG16_Weights\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nfrom collections import defaultdict","metadata":{"id":"x5ee94tk9wcf","outputId":"fa8453f8-e4a0-4e0c-fa41-24aeba9ae9fc","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.557896Z","iopub.execute_input":"2025-06-26T10:31:55.558181Z","iopub.status.idle":"2025-06-26T10:31:55.562629Z","shell.execute_reply.started":"2025-06-26T10:31:55.558160Z","shell.execute_reply":"2025-06-26T10:31:55.561794Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.574654Z","iopub.execute_input":"2025-06-26T10:31:55.574974Z","iopub.status.idle":"2025-06-26T10:31:55.587196Z","shell.execute_reply.started":"2025-06-26T10:31:55.574943Z","shell.execute_reply":"2025-06-26T10:31:55.586485Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"# DATA PREPARATION","metadata":{}},{"cell_type":"code","source":"class MultiInputDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform if transform else get_default_transform()\n        self.samples = []\n        self.labels = []\n        self.label_map = {'KCN':0, 'NOR':1, 'SUSP':2}\n\n        #class loop\n        for class_name in os.listdir(root_dir):\n            class_path = os.path.join(root_dir, class_name)\n            if not os.path.isdir(class_path):\n                continue\n            for case_name in os.listdir(class_path):\n                case_path = os.path.join(class_path, case_name)\n                if os.path.isdir(case_path):\n                    #pick prefix\n                    sample_files = os.listdir(case_path)\n                    if sample_files:\n                        prefix = sample_files[0].split('_')[0] \n                        case_number = sample_files[0].split('_')[1]\n                        case_prefix = f\"{prefix}_{case_number}\"\n                        self.samples.append((case_path, case_prefix, prefix))\n                        self.labels.append(self.label_map[prefix])\n        self.labels = np.array(self.labels)\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        case_path, case_prefix, prefix = self.samples[idx]\n\n        #akhiran nama file\n        feature_suffixes = [\n            \"_CT_A.jpg\", \"_EC_A.jpg\", \"_EC_P.jpg\", \"_Elv_A.jpg\",\"_Elv_P.jpg\", \"_Sag_A.jpg\", \"_Sag_P.jpg\"\n        ]\n\n        feature_images = []\n        for suffix in feature_suffixes :\n            filename = f\"{case_prefix}{suffix}\"\n            img_path = os.path.join(case_path, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            img = self.transform(img)\n            feature_images.append(img)\n\n        #shape : (7, 3, H, W)\n        stacked = torch.stack(feature_images, dim=0)\n        label = self.label_map[prefix]\n        return stacked, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.588121Z","iopub.execute_input":"2025-06-26T10:31:55.588312Z","iopub.status.idle":"2025-06-26T10:31:55.601272Z","shell.execute_reply.started":"2025-06-26T10:31:55.588295Z","shell.execute_reply":"2025-06-26T10:31:55.600436Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def balance_batches(labels, batch_size):\n    try:\n        num_classes = len(np.unique(labels))\n        class_indices = defaultdict(list)\n        for idx, label in enumerate(labels):\n            class_indices[label].append(idx)\n        min_class_len = min(len(idxs) for idxs in class_indices.values())\n        batches = []\n        for i in range (0, min_class_len, batch_size // num_classes) :\n            batch = []\n            for c in range(num_classes) : \n                batch.extend(class_indices[c][i:i + batch_size // num_classes])\n            if len (batch) == batch_size:\n                batches.append(batch)\n        return batches\n    except Exception as e :\n        print(f'Failed to create balance batches : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.602659Z","iopub.execute_input":"2025-06-26T10:31:55.602875Z","iopub.status.idle":"2025-06-26T10:31:55.619462Z","shell.execute_reply.started":"2025-06-26T10:31:55.602857Z","shell.execute_reply":"2025-06-26T10:31:55.618660Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def get_default_transform():\n    try:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    except Exception as e :\n        print(f'Error in transform dataset : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.620565Z","iopub.execute_input":"2025-06-26T10:31:55.620840Z","iopub.status.idle":"2025-06-26T10:31:55.642064Z","shell.execute_reply.started":"2025-06-26T10:31:55.620820Z","shell.execute_reply":"2025-06-26T10:31:55.641392Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def load_dataset(data_dir, transform=None):\n    #create and return teh dataset instance\n    try :\n        dataset = MultiInputDataset(data_dir, transform)\n        print(f'Dataset loaded succesfully with {len(dataset)} samples')\n        return dataset\n        \n    except Exception as e:\n        print(f'Error loading dataset : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.642929Z","iopub.execute_input":"2025-06-26T10:31:55.643202Z","iopub.status.idle":"2025-06-26T10:31:55.661299Z","shell.execute_reply.started":"2025-06-26T10:31:55.643176Z","shell.execute_reply":"2025-06-26T10:31:55.660414Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def create_train_val_split(dataset, val_size=0.2, random_state=42):\n    #splitting dataset with statification\n    try :\n        train_idx, val_idx = train_test_split(\n            range(len(dataset)),\n            test_size = val_size,\n            stratify = dataset.labels,\n            random_state = random_state\n        )\n        print(f'Split created : {len(train_idx)} training samples ; {len(val_idx)} validation samples')\n        return train_idx, val_idx\n        \n    except Exception as e:\n        print(f'Error Splitting : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.661938Z","iopub.execute_input":"2025-06-26T10:31:55.662112Z","iopub.status.idle":"2025-06-26T10:31:55.676205Z","shell.execute_reply.started":"2025-06-26T10:31:55.662095Z","shell.execute_reply":"2025-06-26T10:31:55.675463Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def create_dataloaders(train_dataset, val_dataset, batch_size=8, num_workers=2):\n    #create and return train and val dataloaders\n    try :\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size = batch_size,\n            shuffle = True,\n            num_workers = num_workers,\n            pin_memory = True\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size = batch_size,\n            shuffle = False,\n            num_workers = num_workers,\n            pin_memory = True\n        )\n        print(f'DataLoaders created with batch size {batch_size}')\n        return train_loader, val_loader\n    except Exception as e:\n        print(f'Error creating DataLoaders : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.676893Z","iopub.execute_input":"2025-06-26T10:31:55.677075Z","iopub.status.idle":"2025-06-26T10:31:55.693246Z","shell.execute_reply.started":"2025-06-26T10:31:55.677058Z","shell.execute_reply":"2025-06-26T10:31:55.692534Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# def prepare_dataloaders(data_dir, batch_size=8, test_size=0.2, random_state=42):\n\n#     print(f'Loading dara from : {os.path.abspath(data_dir)}')\n#     transform = get_default_transform()\n#     full_dataset = MultiInputDataset(root_dir = data_dir, transform=transform)\n#     print(f\"Dataset created with {len(full_dataset)} samples\")\n#     if len(full_dataset) == 0:\n#         print(f'WARNING : Dataset is empty!')\n#         return None, None\n\n#     train_idx, val_idx = train_test_split(\n#         list(range(len(full_dataset))),\n#         test_size=test_size,\n#         random_state=random_state,\n#         shuffle=True\n#     )\n\n#     train_dataset = Subset(full_dataset, train_idx)\n#     val_dataset = Subset(full_dataset, val_idx)\n\n#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n#     return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.713875Z","iopub.execute_input":"2025-06-26T10:31:55.714065Z","iopub.status.idle":"2025-06-26T10:31:55.717161Z","shell.execute_reply.started":"2025-06-26T10:31:55.714048Z","shell.execute_reply":"2025-06-26T10:31:55.716406Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def test_sample_batch(loader):\n    try :\n        images, labels = next(iter(loader))\n        print(\"Batch image shape : \", images.shape)\n        print(\"Labels : \", labels)\n        \n    except Exception as e :\n        print(f'Error in testing sample batch : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.731871Z","iopub.execute_input":"2025-06-26T10:31:55.732067Z","iopub.status.idle":"2025-06-26T10:31:55.735891Z","shell.execute_reply.started":"2025-06-26T10:31:55.732050Z","shell.execute_reply":"2025-06-26T10:31:55.735070Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":" # MODEL SETUP","metadata":{}},{"cell_type":"code","source":"def get_model_vgg16(num_classes = 3, device = None):\n    try :\n        print(f'Preparing VGG16 for {num_classes} classes...')\n        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        for param in vgg16.parameters():\n            param.requires_grad = False\n        vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes)\n        for param in vgg16.classifier.parameters():\n            param.requires_grad = True\n        if device is not None:\n            vgg16 = vgg16.to(device)\n        print(f'Model ready!')\n    \n        return vgg16\n    except Exception as e:\n        print(f'Error in model setup : {str(e)}')\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.737043Z","iopub.execute_input":"2025-06-26T10:31:55.737306Z","iopub.status.idle":"2025-06-26T10:31:55.753177Z","shell.execute_reply.started":"2025-06-26T10:31:55.737287Z","shell.execute_reply":"2025-06-26T10:31:55.752361Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def main() : \n    data_dir = \"/kaggle/input/cornealtopography/Train_Validation sets/Train_Validation sets\"\n    try :\n        print(\"Folders : \", os.listdir(data_dir))\n        for class_name in os.listdir(data_dir):\n            class_path = os.path.join(data_dir, class_name)\n            print(f\"{class_name} -> {len(os.listdir(class_path))} case\")\n\n        #data preparation\n        dataset = load_dataset(data_dir)\n        train_idx, val_idx = create_train_val_split(dataset)\n\n        train_dataset = Subset(dataset, train_idx)\n        val_dataset = Subset(dataset, val_idx)\n\n        #balance batches\n        # train_loader, val_loader = create_dataloaders(train_dataset, val_dataset)\n        batches = balance_batches(dataset.labels[train_idx], batch_size=8)\n        train_loader =  DataLoader(Subset(dataset, sum(batches, [])), batch_size=8, shuffle=False)\n        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n        test_sample_batch (train_loader)\n\n        #model setup\n        model = get_model_vgg16(num_classes = 3, device=device)\n        \n        return train_loader, val_loader, model\n\n    except Exception as e :\n        print(f'Error in main workflow : {str(e)}')\n        return None, None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.754495Z","iopub.execute_input":"2025-06-26T10:31:55.754758Z","iopub.status.idle":"2025-06-26T10:31:55.773460Z","shell.execute_reply.started":"2025-06-26T10:31:55.754739Z","shell.execute_reply":"2025-06-26T10:31:55.772648Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"if __name__ == \"__main__\" :\n    traind_loader, val_loader, model = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T10:31:55.774361Z","iopub.execute_input":"2025-06-26T10:31:55.774634Z","iopub.status.idle":"2025-06-26T10:31:56.200922Z","shell.execute_reply.started":"2025-06-26T10:31:55.774588Z","shell.execute_reply":"2025-06-26T10:31:56.200260Z"}},"outputs":[{"name":"stdout","text":"Folders :  ['Keratoconus', 'Normal', 'Suspect']\nKeratoconus -> 150 case\nNormal -> 150 case\nSuspect -> 123 case\nDataset loaded succesfully with 423 samples\nSplit created : 338 training samples ; 85 validation samples\nError in testing sample batch : \nError in main workflow : \n","output_type":"stream"}],"execution_count":53}]}