{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the DataLoader objects\n",
    "with open(r\"C:\\Users\\user\\Documents\\!TA\\!TA\\all trial\\train_loader.pkl\", \"rb\") as f:\n",
    "    train_loader = pickle.load(f)\n",
    "with open(r\"C:\\Users\\user\\Documents\\!TA\\!TA\\all trial\\valid_loader.pkl\", \"rb\") as f:\n",
    "    valid_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader: 37 batches\n",
      "Valid Loader: 10 batches\n"
     ]
    }
   ],
   "source": [
    "#Confirm the DataLoaders are loaded\n",
    "print(f\"Train Loader: {len(train_loader)} batches\")\n",
    "print(f\"Valid Loader: {len(valid_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Check device availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50resnet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer for 3 output classes\n",
    "resnet50.fc = nn.Linear(in_features=resnet50.fc.in_features, out_features=3)  # 3 classes: Keratoconus, Normal, Suspect\n",
    "\n",
    "# Move the model to the appropriate device (GPU/CPU)\n",
    "resnet50 = resnet50.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: requires_grad=False\n",
      "bn1.weight: requires_grad=False\n",
      "bn1.bias: requires_grad=False\n",
      "layer1.0.conv1.weight: requires_grad=False\n",
      "layer1.0.bn1.weight: requires_grad=False\n",
      "layer1.0.bn1.bias: requires_grad=False\n",
      "layer1.0.conv2.weight: requires_grad=False\n",
      "layer1.0.bn2.weight: requires_grad=False\n",
      "layer1.0.bn2.bias: requires_grad=False\n",
      "layer1.0.conv3.weight: requires_grad=False\n",
      "layer1.0.bn3.weight: requires_grad=False\n",
      "layer1.0.bn3.bias: requires_grad=False\n",
      "layer1.0.downsample.0.weight: requires_grad=False\n",
      "layer1.0.downsample.1.weight: requires_grad=False\n",
      "layer1.0.downsample.1.bias: requires_grad=False\n",
      "layer1.1.conv1.weight: requires_grad=False\n",
      "layer1.1.bn1.weight: requires_grad=False\n",
      "layer1.1.bn1.bias: requires_grad=False\n",
      "layer1.1.conv2.weight: requires_grad=False\n",
      "layer1.1.bn2.weight: requires_grad=False\n",
      "layer1.1.bn2.bias: requires_grad=False\n",
      "layer1.1.conv3.weight: requires_grad=False\n",
      "layer1.1.bn3.weight: requires_grad=False\n",
      "layer1.1.bn3.bias: requires_grad=False\n",
      "layer1.2.conv1.weight: requires_grad=False\n",
      "layer1.2.bn1.weight: requires_grad=False\n",
      "layer1.2.bn1.bias: requires_grad=False\n",
      "layer1.2.conv2.weight: requires_grad=False\n",
      "layer1.2.bn2.weight: requires_grad=False\n",
      "layer1.2.bn2.bias: requires_grad=False\n",
      "layer1.2.conv3.weight: requires_grad=False\n",
      "layer1.2.bn3.weight: requires_grad=False\n",
      "layer1.2.bn3.bias: requires_grad=False\n",
      "layer2.0.conv1.weight: requires_grad=False\n",
      "layer2.0.bn1.weight: requires_grad=False\n",
      "layer2.0.bn1.bias: requires_grad=False\n",
      "layer2.0.conv2.weight: requires_grad=False\n",
      "layer2.0.bn2.weight: requires_grad=False\n",
      "layer2.0.bn2.bias: requires_grad=False\n",
      "layer2.0.conv3.weight: requires_grad=False\n",
      "layer2.0.bn3.weight: requires_grad=False\n",
      "layer2.0.bn3.bias: requires_grad=False\n",
      "layer2.0.downsample.0.weight: requires_grad=False\n",
      "layer2.0.downsample.1.weight: requires_grad=False\n",
      "layer2.0.downsample.1.bias: requires_grad=False\n",
      "layer2.1.conv1.weight: requires_grad=False\n",
      "layer2.1.bn1.weight: requires_grad=False\n",
      "layer2.1.bn1.bias: requires_grad=False\n",
      "layer2.1.conv2.weight: requires_grad=False\n",
      "layer2.1.bn2.weight: requires_grad=False\n",
      "layer2.1.bn2.bias: requires_grad=False\n",
      "layer2.1.conv3.weight: requires_grad=False\n",
      "layer2.1.bn3.weight: requires_grad=False\n",
      "layer2.1.bn3.bias: requires_grad=False\n",
      "layer2.2.conv1.weight: requires_grad=False\n",
      "layer2.2.bn1.weight: requires_grad=False\n",
      "layer2.2.bn1.bias: requires_grad=False\n",
      "layer2.2.conv2.weight: requires_grad=False\n",
      "layer2.2.bn2.weight: requires_grad=False\n",
      "layer2.2.bn2.bias: requires_grad=False\n",
      "layer2.2.conv3.weight: requires_grad=False\n",
      "layer2.2.bn3.weight: requires_grad=False\n",
      "layer2.2.bn3.bias: requires_grad=False\n",
      "layer2.3.conv1.weight: requires_grad=False\n",
      "layer2.3.bn1.weight: requires_grad=False\n",
      "layer2.3.bn1.bias: requires_grad=False\n",
      "layer2.3.conv2.weight: requires_grad=False\n",
      "layer2.3.bn2.weight: requires_grad=False\n",
      "layer2.3.bn2.bias: requires_grad=False\n",
      "layer2.3.conv3.weight: requires_grad=False\n",
      "layer2.3.bn3.weight: requires_grad=False\n",
      "layer2.3.bn3.bias: requires_grad=False\n",
      "layer3.0.conv1.weight: requires_grad=False\n",
      "layer3.0.bn1.weight: requires_grad=False\n",
      "layer3.0.bn1.bias: requires_grad=False\n",
      "layer3.0.conv2.weight: requires_grad=False\n",
      "layer3.0.bn2.weight: requires_grad=False\n",
      "layer3.0.bn2.bias: requires_grad=False\n",
      "layer3.0.conv3.weight: requires_grad=False\n",
      "layer3.0.bn3.weight: requires_grad=False\n",
      "layer3.0.bn3.bias: requires_grad=False\n",
      "layer3.0.downsample.0.weight: requires_grad=False\n",
      "layer3.0.downsample.1.weight: requires_grad=False\n",
      "layer3.0.downsample.1.bias: requires_grad=False\n",
      "layer3.1.conv1.weight: requires_grad=False\n",
      "layer3.1.bn1.weight: requires_grad=False\n",
      "layer3.1.bn1.bias: requires_grad=False\n",
      "layer3.1.conv2.weight: requires_grad=False\n",
      "layer3.1.bn2.weight: requires_grad=False\n",
      "layer3.1.bn2.bias: requires_grad=False\n",
      "layer3.1.conv3.weight: requires_grad=False\n",
      "layer3.1.bn3.weight: requires_grad=False\n",
      "layer3.1.bn3.bias: requires_grad=False\n",
      "layer3.2.conv1.weight: requires_grad=False\n",
      "layer3.2.bn1.weight: requires_grad=False\n",
      "layer3.2.bn1.bias: requires_grad=False\n",
      "layer3.2.conv2.weight: requires_grad=False\n",
      "layer3.2.bn2.weight: requires_grad=False\n",
      "layer3.2.bn2.bias: requires_grad=False\n",
      "layer3.2.conv3.weight: requires_grad=False\n",
      "layer3.2.bn3.weight: requires_grad=False\n",
      "layer3.2.bn3.bias: requires_grad=False\n",
      "layer3.3.conv1.weight: requires_grad=False\n",
      "layer3.3.bn1.weight: requires_grad=False\n",
      "layer3.3.bn1.bias: requires_grad=False\n",
      "layer3.3.conv2.weight: requires_grad=False\n",
      "layer3.3.bn2.weight: requires_grad=False\n",
      "layer3.3.bn2.bias: requires_grad=False\n",
      "layer3.3.conv3.weight: requires_grad=False\n",
      "layer3.3.bn3.weight: requires_grad=False\n",
      "layer3.3.bn3.bias: requires_grad=False\n",
      "layer3.4.conv1.weight: requires_grad=False\n",
      "layer3.4.bn1.weight: requires_grad=False\n",
      "layer3.4.bn1.bias: requires_grad=False\n",
      "layer3.4.conv2.weight: requires_grad=False\n",
      "layer3.4.bn2.weight: requires_grad=False\n",
      "layer3.4.bn2.bias: requires_grad=False\n",
      "layer3.4.conv3.weight: requires_grad=False\n",
      "layer3.4.bn3.weight: requires_grad=False\n",
      "layer3.4.bn3.bias: requires_grad=False\n",
      "layer3.5.conv1.weight: requires_grad=False\n",
      "layer3.5.bn1.weight: requires_grad=False\n",
      "layer3.5.bn1.bias: requires_grad=False\n",
      "layer3.5.conv2.weight: requires_grad=False\n",
      "layer3.5.bn2.weight: requires_grad=False\n",
      "layer3.5.bn2.bias: requires_grad=False\n",
      "layer3.5.conv3.weight: requires_grad=False\n",
      "layer3.5.bn3.weight: requires_grad=False\n",
      "layer3.5.bn3.bias: requires_grad=False\n",
      "layer4.0.conv1.weight: requires_grad=False\n",
      "layer4.0.bn1.weight: requires_grad=False\n",
      "layer4.0.bn1.bias: requires_grad=False\n",
      "layer4.0.conv2.weight: requires_grad=False\n",
      "layer4.0.bn2.weight: requires_grad=False\n",
      "layer4.0.bn2.bias: requires_grad=False\n",
      "layer4.0.conv3.weight: requires_grad=False\n",
      "layer4.0.bn3.weight: requires_grad=False\n",
      "layer4.0.bn3.bias: requires_grad=False\n",
      "layer4.0.downsample.0.weight: requires_grad=False\n",
      "layer4.0.downsample.1.weight: requires_grad=False\n",
      "layer4.0.downsample.1.bias: requires_grad=False\n",
      "layer4.1.conv1.weight: requires_grad=False\n",
      "layer4.1.bn1.weight: requires_grad=False\n",
      "layer4.1.bn1.bias: requires_grad=False\n",
      "layer4.1.conv2.weight: requires_grad=False\n",
      "layer4.1.bn2.weight: requires_grad=False\n",
      "layer4.1.bn2.bias: requires_grad=False\n",
      "layer4.1.conv3.weight: requires_grad=False\n",
      "layer4.1.bn3.weight: requires_grad=False\n",
      "layer4.1.bn3.bias: requires_grad=False\n",
      "layer4.2.conv1.weight: requires_grad=False\n",
      "layer4.2.bn1.weight: requires_grad=False\n",
      "layer4.2.bn1.bias: requires_grad=False\n",
      "layer4.2.conv2.weight: requires_grad=False\n",
      "layer4.2.bn2.weight: requires_grad=False\n",
      "layer4.2.bn2.bias: requires_grad=False\n",
      "layer4.2.conv3.weight: requires_grad=False\n",
      "layer4.2.bn3.weight: requires_grad=False\n",
      "layer4.2.bn3.bias: requires_grad=False\n",
      "fc.weight: requires_grad=True\n",
      "fc.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the feature extractor layers (everything except the final classifier layer)\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "# Unfreeze the final classifier layer to fine-tune it\n",
    "for param in resnet50.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print the names of layers and whether they are trainable\n",
    "for name, param in resnet50.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(resnet50.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce lr by 0.5 every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Early stopping criteria\n",
    "# best_accuracy = 0.0\n",
    "# patience = 3  # Stop after 3 epochs with no improvement\n",
    "# epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=100):\n",
    "    # global best_accuracy, epochs_without_improvement\n",
    "    best_accuracy = 0.0 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        # Calculate train accuracy\n",
    "        train_accuracy = 100 * train_correct / total_train\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        valid_correct = 0\n",
    "        total_valid = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                valid_correct += (preds == labels).sum().item()\n",
    "                total_valid += labels.size(0)\n",
    "                \n",
    "        # Calculate validatin accuracy\n",
    "        valid_accuracy = 100 * valid_correct / total_valid\n",
    "\n",
    "        # Print stats for the epoch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "        # Save the model with the best validation accuracy\n",
    "        if valid_accuracy > best_accuracy:\n",
    "            best_accuracy = valid_accuracy\n",
    "            torch.save(model.state_dict(), \"best_resnet50_model.pth\")\n",
    "        #     epochs_without_improvement = 0\n",
    "        # else:\n",
    "        #     epochs_without_improvement += 1\n",
    "\n",
    "        # # Save checkpoints after every 5 epochs\n",
    "        # if (epoch + 1) % 5 == 0:\n",
    "        #     torch.save(model.state_dict(), f\"vgg16_epoch_{epoch+1}.pth\")\n",
    "\n",
    "        # # Stop training early if no improvement\n",
    "        # if epochs_without_improvement >= patience:\n",
    "        #     print(\"Early stopping triggered!\")\n",
    "        #     break\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.9979, Train Accuracy: 48.31%, Validation Accuracy: 58.52%\n",
      "Epoch [2/100], Train Loss: 0.8170, Train Accuracy: 61.78%, Validation Accuracy: 65.26%\n",
      "Epoch [3/100], Train Loss: 0.7522, Train Accuracy: 66.05%, Validation Accuracy: 68.47%\n",
      "Epoch [4/100], Train Loss: 0.7399, Train Accuracy: 65.16%, Validation Accuracy: 67.96%\n",
      "Epoch [5/100], Train Loss: 0.7014, Train Accuracy: 68.54%, Validation Accuracy: 70.32%\n",
      "Epoch [6/100], Train Loss: 0.6758, Train Accuracy: 70.31%, Validation Accuracy: 71.50%\n",
      "Epoch [7/100], Train Loss: 0.6782, Train Accuracy: 70.48%, Validation Accuracy: 72.34%\n",
      "Epoch [8/100], Train Loss: 0.6627, Train Accuracy: 70.48%, Validation Accuracy: 68.13%\n",
      "Epoch [9/100], Train Loss: 0.6716, Train Accuracy: 70.27%, Validation Accuracy: 72.34%\n",
      "Epoch [10/100], Train Loss: 0.6655, Train Accuracy: 70.19%, Validation Accuracy: 71.67%\n",
      "Epoch [11/100], Train Loss: 0.6395, Train Accuracy: 72.13%, Validation Accuracy: 70.66%\n",
      "Epoch [12/100], Train Loss: 0.6334, Train Accuracy: 72.09%, Validation Accuracy: 72.34%\n",
      "Epoch [13/100], Train Loss: 0.6348, Train Accuracy: 72.93%, Validation Accuracy: 69.65%\n",
      "Epoch [14/100], Train Loss: 0.6380, Train Accuracy: 71.88%, Validation Accuracy: 70.49%\n",
      "Epoch [15/100], Train Loss: 0.6344, Train Accuracy: 72.30%, Validation Accuracy: 72.51%\n",
      "Epoch [16/100], Train Loss: 0.6310, Train Accuracy: 72.93%, Validation Accuracy: 72.51%\n",
      "Epoch [17/100], Train Loss: 0.6270, Train Accuracy: 72.93%, Validation Accuracy: 72.68%\n",
      "Epoch [18/100], Train Loss: 0.6311, Train Accuracy: 72.30%, Validation Accuracy: 72.85%\n",
      "Epoch [19/100], Train Loss: 0.6272, Train Accuracy: 72.97%, Validation Accuracy: 71.84%\n",
      "Epoch [20/100], Train Loss: 0.6155, Train Accuracy: 73.69%, Validation Accuracy: 73.19%\n",
      "Epoch [21/100], Train Loss: 0.6132, Train Accuracy: 73.86%, Validation Accuracy: 72.68%\n",
      "Epoch [22/100], Train Loss: 0.6159, Train Accuracy: 73.48%, Validation Accuracy: 73.19%\n",
      "Epoch [23/100], Train Loss: 0.6249, Train Accuracy: 72.76%, Validation Accuracy: 72.51%\n",
      "Epoch [24/100], Train Loss: 0.6145, Train Accuracy: 72.93%, Validation Accuracy: 72.68%\n",
      "Epoch [25/100], Train Loss: 0.6193, Train Accuracy: 73.35%, Validation Accuracy: 72.68%\n",
      "Epoch [26/100], Train Loss: 0.6115, Train Accuracy: 74.07%, Validation Accuracy: 72.68%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     14\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\newenv1\\lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "train_model(resnet50, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
